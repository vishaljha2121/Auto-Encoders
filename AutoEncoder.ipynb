{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv('ml-1m/movies.dat', sep = '::', header= None, engine = 'python', encoding = 'latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.read_csv('ml-1m/users.dat', sep = '::', header= None, engine = 'python', encoding = 'latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv('ml-1m/ratings.dat', sep = '::', header= None, engine = 'python', encoding = 'latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = pd.read_csv('ml-100k/u1.base', delimiter = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set, dtype = 'int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.read_csv('ml-100k/u1.test', delimiter = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = np.array(test_set, dtype = 'int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_users = int(max(max(training_set[:, 0]), max(test_set[:, 0])))\n",
    "nb_movies = int(max(max(training_set[:, 1]), max(test_set[:, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(data):\n",
    "    new_data = []\n",
    "    for id_users in range(1,nb_users + 1):\n",
    "        id_movies = data[:, 1][data[:, 0] == id_users]\n",
    "        id_ratings = data[:, 2][data[:, 0] == id_users]\n",
    "        ratings = np.zeros(nb_movies)\n",
    "        ratings[id_movies - 1] = id_ratings\n",
    "        new_data.append(list(ratings))\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = convert(training_set)\n",
    "test_set = convert(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = torch.FloatTensor(training_set)\n",
    "test_set = torch.FloatTensor(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAE(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(SAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(nb_movies, 20)\n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    "        self.fc3 = nn.Linear(10, 20)\n",
    "        self.fc4 = nn.Linear(20, nb_movies)\n",
    "        self.activation = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "sae = SAE()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(sae.parameters(), lr = 0.01, weight_decay = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  Loss: tensor(1.7715)\n",
      "epoch: 2  Loss: tensor(1.0968)\n",
      "epoch: 3  Loss: tensor(1.0533)\n",
      "epoch: 4  Loss: tensor(1.0383)\n",
      "epoch: 5  Loss: tensor(1.0308)\n",
      "epoch: 6  Loss: tensor(1.0266)\n",
      "epoch: 7  Loss: tensor(1.0239)\n",
      "epoch: 8  Loss: tensor(1.0218)\n",
      "epoch: 9  Loss: tensor(1.0206)\n",
      "epoch: 10  Loss: tensor(1.0194)\n",
      "epoch: 11  Loss: tensor(1.0187)\n",
      "epoch: 12  Loss: tensor(1.0186)\n",
      "epoch: 13  Loss: tensor(1.0178)\n",
      "epoch: 14  Loss: tensor(1.0175)\n",
      "epoch: 15  Loss: tensor(1.0173)\n",
      "epoch: 16  Loss: tensor(1.0169)\n",
      "epoch: 17  Loss: tensor(1.0165)\n",
      "epoch: 18  Loss: tensor(1.0165)\n",
      "epoch: 19  Loss: tensor(1.0161)\n",
      "epoch: 20  Loss: tensor(1.0162)\n",
      "epoch: 21  Loss: tensor(1.0158)\n",
      "epoch: 22  Loss: tensor(1.0158)\n",
      "epoch: 23  Loss: tensor(1.0157)\n",
      "epoch: 24  Loss: tensor(1.0158)\n",
      "epoch: 25  Loss: tensor(1.0156)\n",
      "epoch: 26  Loss: tensor(1.0154)\n",
      "epoch: 27  Loss: tensor(1.0156)\n",
      "epoch: 28  Loss: tensor(1.0149)\n",
      "epoch: 29  Loss: tensor(1.0133)\n",
      "epoch: 30  Loss: tensor(1.0118)\n",
      "epoch: 31  Loss: tensor(1.0096)\n",
      "epoch: 32  Loss: tensor(1.0093)\n",
      "epoch: 33  Loss: tensor(1.0052)\n",
      "epoch: 34  Loss: tensor(1.0057)\n",
      "epoch: 35  Loss: tensor(1.0020)\n",
      "epoch: 36  Loss: tensor(1.0007)\n",
      "epoch: 37  Loss: tensor(0.9969)\n",
      "epoch: 38  Loss: tensor(0.9953)\n",
      "epoch: 39  Loss: tensor(0.9925)\n",
      "epoch: 40  Loss: tensor(0.9923)\n",
      "epoch: 41  Loss: tensor(0.9891)\n",
      "epoch: 42  Loss: tensor(0.9906)\n",
      "epoch: 43  Loss: tensor(0.9879)\n",
      "epoch: 44  Loss: tensor(0.9881)\n",
      "epoch: 45  Loss: tensor(0.9804)\n",
      "epoch: 46  Loss: tensor(0.9806)\n",
      "epoch: 47  Loss: tensor(0.9823)\n",
      "epoch: 48  Loss: tensor(0.9846)\n",
      "epoch: 49  Loss: tensor(0.9846)\n",
      "epoch: 50  Loss: tensor(0.9844)\n",
      "epoch: 51  Loss: tensor(0.9787)\n",
      "epoch: 52  Loss: tensor(0.9787)\n",
      "epoch: 53  Loss: tensor(0.9793)\n",
      "epoch: 54  Loss: tensor(0.9748)\n",
      "epoch: 55  Loss: tensor(0.9725)\n",
      "epoch: 56  Loss: tensor(0.9707)\n",
      "epoch: 57  Loss: tensor(0.9668)\n",
      "epoch: 58  Loss: tensor(0.9676)\n",
      "epoch: 59  Loss: tensor(0.9654)\n",
      "epoch: 60  Loss: tensor(0.9629)\n",
      "epoch: 61  Loss: tensor(0.9623)\n",
      "epoch: 62  Loss: tensor(0.9616)\n",
      "epoch: 63  Loss: tensor(0.9611)\n",
      "epoch: 64  Loss: tensor(0.9581)\n",
      "epoch: 65  Loss: tensor(0.9585)\n",
      "epoch: 66  Loss: tensor(0.9565)\n",
      "epoch: 67  Loss: tensor(0.9541)\n",
      "epoch: 68  Loss: tensor(0.9539)\n",
      "epoch: 69  Loss: tensor(0.9510)\n",
      "epoch: 70  Loss: tensor(0.9502)\n",
      "epoch: 71  Loss: tensor(0.9576)\n",
      "epoch: 72  Loss: tensor(0.9543)\n",
      "epoch: 73  Loss: tensor(0.9542)\n",
      "epoch: 74  Loss: tensor(0.9577)\n",
      "epoch: 75  Loss: tensor(0.9532)\n",
      "epoch: 76  Loss: tensor(0.9532)\n",
      "epoch: 77  Loss: tensor(0.9532)\n",
      "epoch: 78  Loss: tensor(0.9506)\n",
      "epoch: 79  Loss: tensor(0.9512)\n",
      "epoch: 80  Loss: tensor(0.9525)\n",
      "epoch: 81  Loss: tensor(0.9543)\n",
      "epoch: 82  Loss: tensor(0.9513)\n",
      "epoch: 83  Loss: tensor(0.9496)\n",
      "epoch: 84  Loss: tensor(0.9497)\n",
      "epoch: 85  Loss: tensor(0.9485)\n",
      "epoch: 86  Loss: tensor(0.9488)\n",
      "epoch: 87  Loss: tensor(0.9466)\n",
      "epoch: 88  Loss: tensor(0.9472)\n",
      "epoch: 89  Loss: tensor(0.9450)\n",
      "epoch: 90  Loss: tensor(0.9456)\n",
      "epoch: 91  Loss: tensor(0.9429)\n",
      "epoch: 92  Loss: tensor(0.9440)\n",
      "epoch: 93  Loss: tensor(0.9424)\n",
      "epoch: 94  Loss: tensor(0.9433)\n",
      "epoch: 95  Loss: tensor(0.9413)\n",
      "epoch: 96  Loss: tensor(0.9415)\n",
      "epoch: 97  Loss: tensor(0.9410)\n",
      "epoch: 98  Loss: tensor(0.9413)\n",
      "epoch: 99  Loss: tensor(0.9390)\n",
      "epoch: 100  Loss: tensor(0.9398)\n",
      "epoch: 101  Loss: tensor(0.9373)\n",
      "epoch: 102  Loss: tensor(0.9380)\n",
      "epoch: 103  Loss: tensor(0.9367)\n",
      "epoch: 104  Loss: tensor(0.9377)\n",
      "epoch: 105  Loss: tensor(0.9363)\n",
      "epoch: 106  Loss: tensor(0.9374)\n",
      "epoch: 107  Loss: tensor(0.9356)\n",
      "epoch: 108  Loss: tensor(0.9363)\n",
      "epoch: 109  Loss: tensor(0.9349)\n",
      "epoch: 110  Loss: tensor(0.9349)\n",
      "epoch: 111  Loss: tensor(0.9333)\n",
      "epoch: 112  Loss: tensor(0.9346)\n",
      "epoch: 113  Loss: tensor(0.9331)\n",
      "epoch: 114  Loss: tensor(0.9333)\n",
      "epoch: 115  Loss: tensor(0.9317)\n",
      "epoch: 116  Loss: tensor(0.9329)\n",
      "epoch: 117  Loss: tensor(0.9311)\n",
      "epoch: 118  Loss: tensor(0.9319)\n",
      "epoch: 119  Loss: tensor(0.9301)\n",
      "epoch: 120  Loss: tensor(0.9319)\n",
      "epoch: 121  Loss: tensor(0.9308)\n",
      "epoch: 122  Loss: tensor(0.9302)\n",
      "epoch: 123  Loss: tensor(0.9292)\n",
      "epoch: 124  Loss: tensor(0.9300)\n",
      "epoch: 125  Loss: tensor(0.9289)\n",
      "epoch: 126  Loss: tensor(0.9295)\n",
      "epoch: 127  Loss: tensor(0.9277)\n",
      "epoch: 128  Loss: tensor(0.9285)\n",
      "epoch: 129  Loss: tensor(0.9273)\n",
      "epoch: 130  Loss: tensor(0.9280)\n",
      "epoch: 131  Loss: tensor(0.9264)\n",
      "epoch: 132  Loss: tensor(0.9279)\n",
      "epoch: 133  Loss: tensor(0.9259)\n",
      "epoch: 134  Loss: tensor(0.9268)\n",
      "epoch: 135  Loss: tensor(0.9250)\n",
      "epoch: 136  Loss: tensor(0.9259)\n",
      "epoch: 137  Loss: tensor(0.9252)\n",
      "epoch: 138  Loss: tensor(0.9261)\n",
      "epoch: 139  Loss: tensor(0.9245)\n",
      "epoch: 140  Loss: tensor(0.9255)\n",
      "epoch: 141  Loss: tensor(0.9241)\n",
      "epoch: 142  Loss: tensor(0.9251)\n",
      "epoch: 143  Loss: tensor(0.9238)\n",
      "epoch: 144  Loss: tensor(0.9258)\n",
      "epoch: 145  Loss: tensor(0.9233)\n",
      "epoch: 146  Loss: tensor(0.9249)\n",
      "epoch: 147  Loss: tensor(0.9230)\n",
      "epoch: 148  Loss: tensor(0.9240)\n",
      "epoch: 149  Loss: tensor(0.9220)\n",
      "epoch: 150  Loss: tensor(0.9232)\n",
      "epoch: 151  Loss: tensor(0.9218)\n",
      "epoch: 152  Loss: tensor(0.9222)\n",
      "epoch: 153  Loss: tensor(0.9219)\n",
      "epoch: 154  Loss: tensor(0.9221)\n",
      "epoch: 155  Loss: tensor(0.9209)\n",
      "epoch: 156  Loss: tensor(0.9216)\n",
      "epoch: 157  Loss: tensor(0.9207)\n",
      "epoch: 158  Loss: tensor(0.9214)\n",
      "epoch: 159  Loss: tensor(0.9201)\n",
      "epoch: 160  Loss: tensor(0.9210)\n",
      "epoch: 161  Loss: tensor(0.9194)\n",
      "epoch: 162  Loss: tensor(0.9206)\n",
      "epoch: 163  Loss: tensor(0.9193)\n",
      "epoch: 164  Loss: tensor(0.9203)\n",
      "epoch: 165  Loss: tensor(0.9193)\n",
      "epoch: 166  Loss: tensor(0.9199)\n",
      "epoch: 167  Loss: tensor(0.9185)\n",
      "epoch: 168  Loss: tensor(0.9197)\n",
      "epoch: 169  Loss: tensor(0.9186)\n",
      "epoch: 170  Loss: tensor(0.9193)\n",
      "epoch: 171  Loss: tensor(0.9180)\n",
      "epoch: 172  Loss: tensor(0.9188)\n",
      "epoch: 173  Loss: tensor(0.9177)\n",
      "epoch: 174  Loss: tensor(0.9187)\n",
      "epoch: 175  Loss: tensor(0.9177)\n",
      "epoch: 176  Loss: tensor(0.9183)\n",
      "epoch: 177  Loss: tensor(0.9175)\n",
      "epoch: 178  Loss: tensor(0.9182)\n",
      "epoch: 179  Loss: tensor(0.9171)\n",
      "epoch: 180  Loss: tensor(0.9182)\n",
      "epoch: 181  Loss: tensor(0.9169)\n",
      "epoch: 182  Loss: tensor(0.9174)\n",
      "epoch: 183  Loss: tensor(0.9166)\n",
      "epoch: 184  Loss: tensor(0.9176)\n",
      "epoch: 185  Loss: tensor(0.9164)\n",
      "epoch: 186  Loss: tensor(0.9169)\n",
      "epoch: 187  Loss: tensor(0.9158)\n",
      "epoch: 188  Loss: tensor(0.9168)\n",
      "epoch: 189  Loss: tensor(0.9156)\n",
      "epoch: 190  Loss: tensor(0.9161)\n",
      "epoch: 191  Loss: tensor(0.9152)\n",
      "epoch: 192  Loss: tensor(0.9160)\n",
      "epoch: 193  Loss: tensor(0.9149)\n",
      "epoch: 194  Loss: tensor(0.9156)\n",
      "epoch: 195  Loss: tensor(0.9149)\n",
      "epoch: 196  Loss: tensor(0.9158)\n",
      "epoch: 197  Loss: tensor(0.9148)\n",
      "epoch: 198  Loss: tensor(0.9151)\n",
      "epoch: 199  Loss: tensor(0.9145)\n",
      "epoch: 200  Loss: tensor(0.9151)\n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 200\n",
    "for epoch in range(1, nb_epoch+1):\n",
    "    train_loss = 0.\n",
    "    s = 0.\n",
    "    for id_user in range(nb_users):\n",
    "        input = Variable(training_set[id_user]).unsqueeze(0)\n",
    "        target = input.clone()\n",
    "        if torch.sum(target.data > 0) > 0:\n",
    "            output = sae(input)\n",
    "            target.require_grad = False\n",
    "            output[target == 0] = 0\n",
    "            loss = criterion(output, target)\n",
    "            mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
    "            loss.backward()\n",
    "            train_loss += np.sqrt(loss.data*mean_corrector)\n",
    "            s += 1.\n",
    "            optimizer.step()\n",
    "    print('epoch: '+str(epoch)+'  Loss: '+str(train_loss/s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.9597)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vishaljha/anaconda3/lib/python3.8/site-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([1682])) that is different to the input size (torch.Size([1, 1682])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0.\n",
    "s = 0.\n",
    "for id_user in range(nb_users):\n",
    "    input = Variable(training_set[id_user]).unsqueeze(0)\n",
    "    target = Variable(test_set[id_user])\n",
    "    if torch.sum(target.data > 0) > 0:\n",
    "        output = sae(input)\n",
    "        target.require_grad = False\n",
    "        output[(target == 0).unsqueeze(0)] = 0\n",
    "        loss = criterion(output, target)\n",
    "        mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
    "        test_loss += np.sqrt(loss.data*mean_corrector)\n",
    "        s += 1.\n",
    "print('Loss: '+str(test_loss/s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
